/public/home/cs177h/zhujl/miniconda3/envs/cs177/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
Epoch: 0        Training Loss: 0.032991
Epoch: 1        Training Loss: 0.029817
Epoch: 2        Training Loss: 0.027035
Epoch: 3        Training Loss: 0.025251
Epoch: 4        Training Loss: 0.023673
Epoch: 5        Training Loss: 0.022647
Epoch: 6        Training Loss: 0.021545
Epoch: 7        Training Loss: 0.020707
Epoch: 8        Training Loss: 0.019748
Epoch: 9        Training Loss: 0.018982
Epoch: 10       Training Loss: 0.017907
Epoch: 11       Training Loss: 0.017307
Epoch: 12       Training Loss: 0.016451
Epoch: 13       Training Loss: 0.015856
Epoch: 14       Training Loss: 0.015248
Epoch: 15       Training Loss: 0.014578
Epoch: 16       Training Loss: 0.014064
Epoch: 17       Training Loss: 0.013361
Epoch: 18       Training Loss: 0.013016
Epoch: 19       Training Loss: 0.012566
Number of training 20
Pass rate:0.8024505708716235
Train version: 0
Test version: 0
(cs177) python MLP.py 
/public/home/cs177h/zhujl/miniconda3/envs/cs177/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
Epoch: 0        Training Loss: 0.033610
Epoch: 1        Training Loss: 0.029457
Epoch: 2        Training Loss: 0.026857
Epoch: 3        Training Loss: 0.025023
Epoch: 4        Training Loss: 0.023621
Epoch: 5        Training Loss: 0.022431
Epoch: 6        Training Loss: 0.021397
Epoch: 7        Training Loss: 0.020319
Epoch: 8        Training Loss: 0.019597
Epoch: 9        Training Loss: 0.018783
Epoch: 10       Training Loss: 0.017934
Epoch: 11       Training Loss: 0.016888
Epoch: 12       Training Loss: 0.016156
Epoch: 13       Training Loss: 0.015356
Epoch: 14       Training Loss: 0.014732
Epoch: 15       Training Loss: 0.014038
Epoch: 16       Training Loss: 0.013522
Epoch: 17       Training Loss: 0.012949
Epoch: 18       Training Loss: 0.012531
Epoch: 19       Training Loss: 0.012124
Epoch: 20       Training Loss: 0.011560
Epoch: 21       Training Loss: 0.011399
Epoch: 22       Training Loss: 0.011037
Epoch: 23       Training Loss: 0.010713
Epoch: 24       Training Loss: 0.010353
Epoch: 25       Training Loss: 0.010141
Epoch: 26       Training Loss: 0.009873
Epoch: 27       Training Loss: 0.009573
Epoch: 28       Training Loss: 0.009303
Epoch: 29       Training Loss: 0.009078
Epoch: 30       Training Loss: 0.008781
Epoch: 31       Training Loss: 0.008653
Epoch: 32       Training Loss: 0.008392
Epoch: 33       Training Loss: 0.008289
Epoch: 34       Training Loss: 0.008040
Epoch: 35       Training Loss: 0.007962
Epoch: 36       Training Loss: 0.007708
Epoch: 37       Training Loss: 0.007589
Epoch: 38       Training Loss: 0.007465
Epoch: 39       Training Loss: 0.007246
Epoch: 40       Training Loss: 0.007105
Epoch: 41       Training Loss: 0.007014
Epoch: 42       Training Loss: 0.006825
Epoch: 43       Training Loss: 0.006665
Epoch: 44       Training Loss: 0.006578
Epoch: 45       Training Loss: 0.006361
Epoch: 46       Training Loss: 0.006215
Epoch: 47       Training Loss: 0.006183
Epoch: 48       Training Loss: 0.006043
Epoch: 49       Training Loss: 0.005871
Number of training 50
Pass rate:0.8257309941520468
Train version: 0
Test version: 0